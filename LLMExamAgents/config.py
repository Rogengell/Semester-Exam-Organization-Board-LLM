LLM_CONFIG = {
    "config_list": [
        {
            "model": "llama3.1:8b",
            "client_host": "http://host.docker.internal:11434",
            "api_type": "ollama",
            "temperature": 0.0,
            "stream": False,
        }
    ]
}