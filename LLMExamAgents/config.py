LLM_CONFIG = {
    "config_list": [
        {
            "model": "llama3.1:8b",
            "api_key": "vZTO5AQxbVq4Vzlm7eJ1jrqS3s2P3rmD",
            "api_type": "ollama",
            "api_rate_limit": 0.25,
            "temperature": 0.0,
            "stream": False,
        }
    ]
}